{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-14 10:23:04.034910: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import pickle\n",
    "from VALIDATION.KeyRef_2.KeyRef_2_env import KeyRef2_Env\n",
    "from stable_baselines3.common.callbacks   import BaseCallback\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "K = 30\n",
    "planning_horizon = 480*60\n",
    "ReworkProbability = 0.03\n",
    "\n",
    "with open('VALIDATION/SMALL/pickle_JA_valid_scenarios_480.pkl', 'rb') as f:\n",
    "    valid_scenarios = pickle.load(f)\n",
    "\n",
    "env = KeyRef2_Env(K, planning_horizon, ReworkProbability, valid_scenarios)\n",
    "\n",
    "# check_env(env)\n",
    "# obs = env.reset(seed=42)\n",
    "# print(\"Observation:\", obs)\n",
    "\n",
    "# episodes = 2\n",
    "# for episode in range(episodes):\n",
    "# \tdone = False\n",
    "# \tobs = env.reset()\n",
    "# \twhile done == False:#not done:\n",
    "# \t\trandom_action = env.action_space.sample()\n",
    "# \t\tobs, reward, done, truncated, info = env.step(random_action)\n",
    "# \t\tprint('reward', reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to logs/keyref2-2024-07-14_10-23-05/KeyRef2_1\n",
      "-- tard 468230.0\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 3.18e+03  |\n",
      "|    ep_rew_mean      | -2.67e+03 |\n",
      "|    exploration_rate | 0.652     |\n",
      "| time/               |           |\n",
      "|    episodes         | 1         |\n",
      "|    fps              | 41        |\n",
      "|    time_elapsed     | 76        |\n",
      "|    total_timesteps  | 3177      |\n",
      "| train/              |           |\n",
      "|    episode_reward   | 1         |\n",
      "|    loss             | 0.15      |\n",
      "-----------------------------------\n",
      "-- tard 877330.0\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 3.19e+03  |\n",
      "|    ep_rew_mean      | -2.72e+03 |\n",
      "|    exploration_rate | 0.502     |\n",
      "| time/               |           |\n",
      "|    episodes         | 2         |\n",
      "|    fps              | 37        |\n",
      "|    time_elapsed     | 168       |\n",
      "|    total_timesteps  | 6375      |\n",
      "| train/              |           |\n",
      "|    episode_reward   | 1         |\n",
      "|    loss             | 0.0894    |\n",
      "-----------------------------------\n",
      "-- tard 2259430.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.39e+03 |\n",
      "|    ep_rew_mean      | -2.4e+03 |\n",
      "|    exploration_rate | 0.326    |\n",
      "| time/               |          |\n",
      "|    episodes         | 3        |\n",
      "|    fps              | 33       |\n",
      "|    time_elapsed     | 302      |\n",
      "|    total_timesteps  | 10166    |\n",
      "| train/              |          |\n",
      "|    episode_reward   | 0        |\n",
      "|    loss             | 0.0566   |\n",
      "----------------------------------\n",
      "-- tard 2650840.0\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 2.93e+03  |\n",
      "|    ep_rew_mean      | -1.82e+03 |\n",
      "|    exploration_rate | 0.253     |\n",
      "| time/               |           |\n",
      "|    episodes         | 4         |\n",
      "|    fps              | 36        |\n",
      "|    time_elapsed     | 322       |\n",
      "|    total_timesteps  | 11713     |\n",
      "| train/              |           |\n",
      "|    episode_reward   | 6         |\n",
      "|    loss             | 0.117     |\n",
      "-----------------------------------\n",
      "-- tard 2546950.0\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 2.67e+03  |\n",
      "|    ep_rew_mean      | -1.66e+03 |\n",
      "|    exploration_rate | 0.176     |\n",
      "| time/               |           |\n",
      "|    episodes         | 5         |\n",
      "|    fps              | 38        |\n",
      "|    time_elapsed     | 349       |\n",
      "|    total_timesteps  | 13369     |\n",
      "| train/              |           |\n",
      "|    episode_reward   | -8        |\n",
      "|    loss             | 0.144     |\n",
      "-----------------------------------\n",
      "-- tard 3910330.0\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 2.42e+03  |\n",
      "|    ep_rew_mean      | -1.43e+03 |\n",
      "|    exploration_rate | 0.123     |\n",
      "| time/               |           |\n",
      "|    episodes         | 6         |\n",
      "|    fps              | 40        |\n",
      "|    time_elapsed     | 360       |\n",
      "|    total_timesteps  | 14499     |\n",
      "| train/              |           |\n",
      "|    episode_reward   | 0         |\n",
      "|    loss             | 0.189     |\n",
      "-----------------------------------\n",
      "-- tard 3542900.0\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 2.31e+03  |\n",
      "|    ep_rew_mean      | -1.36e+03 |\n",
      "|    exploration_rate | 0.1       |\n",
      "| time/               |           |\n",
      "|    episodes         | 7         |\n",
      "|    fps              | 41        |\n",
      "|    time_elapsed     | 389       |\n",
      "|    total_timesteps  | 16151     |\n",
      "| train/              |           |\n",
      "|    episode_reward   | 7         |\n",
      "|    loss             | 0.11      |\n",
      "-----------------------------------\n",
      "-- tard 8500610.0\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 2.53e+03  |\n",
      "|    ep_rew_mean      | -1.47e+03 |\n",
      "|    exploration_rate | 0.1       |\n",
      "| time/               |           |\n",
      "|    episodes         | 8         |\n",
      "|    fps              | 37        |\n",
      "|    time_elapsed     | 545       |\n",
      "|    total_timesteps  | 20242     |\n",
      "| train/              |           |\n",
      "|    episode_reward   | 10        |\n",
      "|    loss             | 0.204     |\n",
      "-----------------------------------\n",
      "-- tard 3835920.0\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 2.41e+03  |\n",
      "|    ep_rew_mean      | -1.33e+03 |\n",
      "|    exploration_rate | 0.1       |\n",
      "| time/               |           |\n",
      "|    episodes         | 9         |\n",
      "|    fps              | 38        |\n",
      "|    time_elapsed     | 564       |\n",
      "|    total_timesteps  | 21728     |\n",
      "| train/              |           |\n",
      "|    episode_reward   | 0         |\n",
      "|    loss             | 0.152     |\n",
      "-----------------------------------\n",
      "-- tard 1921240.0\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 2.4e+03   |\n",
      "|    ep_rew_mean      | -1.37e+03 |\n",
      "|    exploration_rate | 0.1       |\n",
      "| time/               |           |\n",
      "|    episodes         | 10        |\n",
      "|    fps              | 39        |\n",
      "|    time_elapsed     | 613       |\n",
      "|    total_timesteps  | 24032     |\n",
      "| train/              |           |\n",
      "|    episode_reward   | 5         |\n",
      "|    loss             | 0.142     |\n",
      "-----------------------------------\n",
      "-- tard 2222020.0\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 2.34e+03  |\n",
      "|    ep_rew_mean      | -1.35e+03 |\n",
      "|    exploration_rate | 0.1       |\n",
      "| time/               |           |\n",
      "|    episodes         | 11        |\n",
      "|    fps              | 40        |\n",
      "|    time_elapsed     | 637       |\n",
      "|    total_timesteps  | 25688     |\n",
      "| train/              |           |\n",
      "|    episode_reward   | -1        |\n",
      "|    loss             | 0.214     |\n",
      "-----------------------------------\n",
      "-- tard 110310.0\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.27e+03 |\n",
      "|    ep_rew_mean      | -1.3e+03 |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 41       |\n",
      "|    time_elapsed     | 662      |\n",
      "|    total_timesteps  | 27254    |\n",
      "| train/              |          |\n",
      "|    episode_reward   | 8        |\n",
      "|    loss             | 0.174    |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "action_list = [\"CDR1\", \"CDR2\", \"CDR3\", \"CDR4\", \"CDR5\", \"CDR6\"]\n",
    "                          \n",
    "# Create directories for models and logs\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "models_dir = f\"models/keyref2-{current_time}\"\n",
    "logdir = f\"logs/keyref2-{current_time}\"\n",
    "log_training_txt_dir = \"keyref2_log_training_txt\"\n",
    "log_training_excel_dir = \"keyref2_log_training_excel\"\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)\n",
    "if not os.path.exists(log_training_txt_dir):\n",
    "    os.makedirs(log_training_txt_dir)\n",
    "if not os.path.exists(log_training_excel_dir):\n",
    "    os.makedirs(log_training_excel_dir)\n",
    "\n",
    "# Generate unique file names based on current time\n",
    "log_file          = os.path.join(log_training_txt_dir,   f\"training_keyref2.txt\")\n",
    "excel_file        = os.path.join(log_training_excel_dir, f\"training_keyref2.xlsx\")\n",
    "action_count_file = os.path.join(log_training_txt_dir,   f\"action_count_keyref2.txt\")\n",
    "action_excel_file = os.path.join(log_training_excel_dir, f\"action_count_keyref2.xlsx\")\n",
    "\n",
    "# Define the custom callback -------------------------------------------------------------\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, log_dir, excel_file, txt_file, action_count_file, action_excel_file, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.log_dir = log_dir\n",
    "        self.excel_file = excel_file\n",
    "        self.txt_file = txt_file\n",
    "        self.action_count_file = action_count_file\n",
    "        self.action_excel_file = action_excel_file\n",
    "        self.logs = []\n",
    "        self.episode_rewards = []\n",
    "        self.action_counts = {}\n",
    "        self.episode_start = True\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        # Initialize action counts\n",
    "        self.action_counts = {action: 0 for action in action_list}\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.episode_start:\n",
    "            self.episode_rewards.append(0)\n",
    "            self.episode_start = False\n",
    "\n",
    "        # Record reward for the current step\n",
    "        reward = self.locals['rewards'][0]\n",
    "        self.episode_rewards[-1] += reward\n",
    "\n",
    "        # Increment action count\n",
    "        action = self.locals.get('actions', None)\n",
    "        if action is not None:\n",
    "            action_name = action_list[action[0]]\n",
    "            self.action_counts[action_name] += 1\n",
    "            \n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        # Called at the end of each episode\n",
    "        sum_reward   = self.episode_rewards[-1] if self.episode_rewards else 0\n",
    "        \n",
    "        self.logger.record('train/episode_reward',   sum_reward)\n",
    "        self.logs.append({\n",
    "            'sum_reward': sum_reward,\n",
    "        })\n",
    "\n",
    "        self.episode_start = True\n",
    "\n",
    "    \n",
    "    def _on_training_end(self) -> None:\n",
    "        # Save logs to Excel\n",
    "        df = pd.DataFrame(self.logs)\n",
    "        df.to_excel(self.excel_file, index=False)\n",
    "\n",
    "        action_df = pd.DataFrame(list(self.action_counts.items()), columns=['Action', 'Count'])\n",
    "        action_df.to_excel(self.action_excel_file, index=False)\n",
    "\n",
    "        # Save logs to text file\n",
    "        with open(self.txt_file, 'w') as f:\n",
    "            f.write(df.to_string(index=False))\n",
    "        with open(self.action_count_file, 'w') as f:\n",
    "            f.write(action_df.to_string(index=False))\n",
    "\n",
    "# Create the callback\n",
    "callback = CustomCallback(log_dir=logdir, \n",
    "                          excel_file=excel_file,\n",
    "                          txt_file=log_file,\n",
    "                          action_count_file=action_count_file,\n",
    "                          action_excel_file=action_excel_file,\n",
    "                          verbose=1)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "class CustomFeaturesExtractor(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    Custom feature extractor for DQN.\n",
    "    \n",
    "    :param observation_space: (spaces.Box)\n",
    "    \"\"\"\n",
    "    def __init__(self, observation_space: spaces.Box):\n",
    "        super(CustomFeaturesExtractor, self).__init__(observation_space, features_dim=6)  # Output features_dim matches last layer's output\n",
    "        n_input_nodes = observation_space.shape[0]\n",
    "        self.fc1 = nn.Linear(n_input_nodes, 30)\n",
    "        self.fc2 = nn.Linear(30, 30)\n",
    "        self.fc3 = nn.Linear(30, 30)\n",
    "        self.fc4 = nn.Linear(30, 30)\n",
    "        self.fc5 = nn.Linear(30, 30)\n",
    "        self.fc6 = nn.Linear(30, 6)  # Output layer with 6 nodes\n",
    "\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.tanh(self.fc1(observations))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = F.tanh(self.fc3(x))\n",
    "        x = F.tanh(self.fc4(x))\n",
    "        x = F.tanh(self.fc5(x))\n",
    "        x = self.fc6(x)  # Output layer\n",
    "        return x\n",
    "\n",
    "# Custom DQN class to implement soft target update\n",
    "class CustomDQN(DQN):\n",
    "    def __init__(self, *args, tau=0.01, **kwargs):\n",
    "        super(CustomDQN, self).__init__(*args, **kwargs)\n",
    "        self.tau = tau\n",
    "\n",
    "    def train(self, gradient_steps, batch_size=100):\n",
    "        # Train for gradient_steps\n",
    "        for gradient_step in range(gradient_steps):\n",
    "            # Sample replay buffer\n",
    "            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n",
    "            \n",
    "            # Mix online and target networks\n",
    "            target_q_values = self.q_net_target(replay_data.next_observations)\n",
    "            next_q_values, _ = target_q_values.max(dim=1)\n",
    "            next_q_values = next_q_values.reshape(-1, 1)\n",
    "\n",
    "            # Compute the target for the Q function\n",
    "            target_q = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n",
    "\n",
    "            # Get current Q estimates\n",
    "            current_q = self.q_net(replay_data.observations).gather(1, replay_data.actions.long())\n",
    "\n",
    "            # Compute Huber loss (less sensitive to outliers)\n",
    "            loss = F.smooth_l1_loss(current_q, target_q)\n",
    "\n",
    "            # Optimize the model\n",
    "            self.policy.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.policy.optimizer.step()\n",
    "\n",
    "            # Soft update of target network\n",
    "            with torch.no_grad():\n",
    "                for target_param, param in zip(self.q_net_target.parameters(), self.q_net.parameters()):\n",
    "                    target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n",
    "\n",
    "            self.logger.record('train/loss', loss.item())\n",
    "\n",
    "# Define policy_kwargs for DQN model\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomFeaturesExtractor,\n",
    ")\n",
    "\n",
    "model_path = os.path.join(models_dir, \"CustomDQN_.zip\")\n",
    "# Initialize CustomDQN using the custom model\n",
    "model = CustomDQN(\n",
    "    'MlpPolicy',                    # Use a Multi-layer Perceptron (MLP) policy\n",
    "    env,                            # Your RL environment\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    buffer_size=1000000,            # Replay buffer size N\n",
    "    batch_size=64,                  # Batch size\n",
    "    gamma=0.9,                      # Discount factor\n",
    "    tau=0.01,                       # Soft target update strategy\n",
    "    exploration_initial_eps=0.8,    # Initial epsilon\n",
    "    exploration_final_eps=0.1,      # Final epsilon\n",
    "    exploration_fraction=0.5,\n",
    "    verbose=1,\n",
    "    tensorboard_log=logdir,\n",
    "    train_freq=(10,\"step\"),\n",
    "    learning_starts= 2000,\n",
    "    learning_rate= 1e-4\n",
    ")\n",
    "model.learn(total_timesteps=30000, \n",
    "            tb_log_name=\"KeyRef2\",\n",
    "            log_interval=1,\n",
    "            reset_num_timesteps=True,\n",
    "            callback=callback)\n",
    "model.save(model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------- valid1\n",
      "----- A\n",
      "-- tard 2678530.0\n",
      "2678530.0\n",
      "----- B\n",
      "-- tard 3219000.0\n",
      "3219000.0\n",
      "----- C\n",
      "-- tard 3650920.0\n",
      "3650920.0\n",
      "----------- valid2\n",
      "----- A\n",
      "-- tard 638170.0\n",
      "638170.0\n",
      "----- B\n",
      "-- tard 368920.0\n",
      "368920.0\n",
      "----- C\n",
      "-- tard 327310.0\n",
      "327310.0\n",
      "----------- valid3\n",
      "----- A\n",
      "-- tard 1550810.0\n",
      "1550810.0\n",
      "----- B\n",
      "-- tard 1669930.0\n",
      "1669930.0\n",
      "----- C\n",
      "-- tard 1458070.0\n",
      "1458070.0\n",
      "----------- valid4\n",
      "----- A\n",
      "-- tard 1414580.0\n",
      "1414580.0\n",
      "----- B\n",
      "-- tard 1062660.0\n",
      "1062660.0\n",
      "----- C\n",
      "-- tard 437320.0\n",
      "437320.0\n",
      "----------- valid5\n",
      "----- A\n",
      "-- tard 942390.0\n",
      "942390.0\n",
      "----- B\n",
      "-- tard 907440.0\n",
      "907440.0\n",
      "----- C\n",
      "-- tard 1216560.0\n",
      "1216560.0\n",
      "----------- valid6\n",
      "----- A\n",
      "-- tard 7920140.0\n",
      "7920140.0\n",
      "----- B\n",
      "-- tard 6081190.0\n",
      "6081190.0\n",
      "----- C\n",
      "-- tard 5791250.0\n",
      "5791250.0\n",
      "----------- valid7\n",
      "----- A\n",
      "-- tard 1121160.0\n",
      "1121160.0\n",
      "----- B\n",
      "-- tard 818210.0\n",
      "818210.0\n",
      "----- C\n",
      "-- tard 686230.0\n",
      "686230.0\n",
      "----------- valid8\n",
      "----- A\n",
      "-- tard 2034740.0\n",
      "2034740.0\n",
      "----- B\n",
      "-- tard 1821510.0\n",
      "1821510.0\n",
      "----- C\n",
      "-- tard 2814700.0\n",
      "2814700.0\n",
      "----------- valid9\n",
      "----- A\n",
      "-- tard 2173570.0\n",
      "2173570.0\n",
      "----- B\n",
      "-- tard 1846580.0\n",
      "1846580.0\n",
      "----- C\n",
      "-- tard 1481380.0\n",
      "1481380.0\n",
      "----------- valid10\n",
      "----- A\n",
      "-- tard 826640.0\n",
      "826640.0\n",
      "----- B\n",
      "-- tard 324210.0\n",
      "324210.0\n",
      "----- C\n",
      "-- tard 328060.0\n",
      "328060.0\n",
      "----------- valid11\n",
      "----- A\n",
      "-- tard 1217190.0\n",
      "1217190.0\n",
      "----- B\n",
      "-- tard 1098730.0\n",
      "1098730.0\n",
      "----- C\n",
      "-- tard 1296780.0\n",
      "1296780.0\n",
      "----------- valid12\n",
      "----- A\n",
      "-- tard 1142300.0\n",
      "1142300.0\n",
      "----- B\n",
      "-- tard 880730.0\n",
      "880730.0\n",
      "----- C\n",
      "-- tard 660720.0\n",
      "660720.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "model = DQN.load(model_path, env=env)\n",
    "def softmax_action_selection(model, obs, mu):\n",
    "    obs_tensor      = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(model.device)\n",
    "    q_values        = model.q_net(obs_tensor).detach().cpu().numpy().flatten()\n",
    "    exp_q_values    = np.exp(mu * q_values)\n",
    "    probabilities   = exp_q_values / np.sum(exp_q_values)\n",
    "    action          = np.random.choice(len(q_values), p=probabilities)\n",
    "    return action\n",
    "\n",
    "results = []\n",
    "method = 'keyref2'\n",
    "InstanceList = [f'valid{i+1}' for i in range(12)]\n",
    "ScenarioList = ['A', 'B', 'C']\n",
    "\n",
    "mu = 1.6 \n",
    "for instance_id in InstanceList:\n",
    "    print(\"-----------\", instance_id)\n",
    "    for scenario_id in ScenarioList:\n",
    "        print(\"-----\", scenario_id)\n",
    "        # Reset the environment with the new dataset\n",
    "        obs, info = env.reset(test=True, \n",
    "                  datatest=instance_id, \n",
    "                  scenariotest=scenario_id)\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = softmax_action_selection(model, obs, mu)\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        tardiness = env.calc_tardiness()\n",
    "        print(tardiness)\n",
    "        results.append({\n",
    "                        'Method'    : method,\n",
    "                        'InstanceID': instance_id,\n",
    "                        'ScenarioID': scenario_id,\n",
    "                        'Tardiness' : tardiness\n",
    "                        })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "file_name = f\"VALIDATION/keyref2.xlsx\"\n",
    "df.to_excel(file_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dunnbebes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import pickle\n",
    "from VALIDATION.KeyRef_2.KeyRef_2_env import KeyRef2_Env\n",
    "from stable_baselines3.common.vec_env     import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks   import BaseCallback, EvalCallback\n",
    "from stable_baselines3.common.monitor     import Monitor\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import DQN\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "K = 30\n",
    "planning_horizon = 480*60\n",
    "ReworkProbability = 0.03\n",
    "\n",
    "with open('VALIDATION/SMALL/pickle_JA_valid_scenarios_480.pkl', 'rb') as f:\n",
    "    valid_scenarios = pickle.load(f)\n",
    "\n",
    "env = KeyRef2_Env(K, planning_horizon, ReworkProbability, valid_scenarios)\n",
    "\n",
    "# check_env(env)\n",
    "# obs = env.reset(seed=42)\n",
    "# print(\"Observation:\", obs)\n",
    "\n",
    "# episodes = 2\n",
    "# for episode in range(episodes):\n",
    "# \tdone = False\n",
    "# \tobs = env.reset()\n",
    "# \twhile done == False:#not done:\n",
    "# \t\trandom_action = env.action_space.sample()\n",
    "# \t\tobs, reward, done, truncated, info = env.step(random_action)\n",
    "# \t\tprint('reward', reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "action_list = [\"CDR1\", \"CDR2\", \"CDR3\", \"CDR4\", \"CDR5\", \"CDR6\"]\n",
    "                          \n",
    "# Create directories for models and logs\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "models_dir = f\"models/keyref2-{current_time}\"\n",
    "logdir = f\"logs/keyref2-{current_time}\"\n",
    "log_training_txt_dir = \"keyref2_log_training_txt\"\n",
    "log_training_excel_dir = \"keyref2_log_training_excel\"\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)\n",
    "if not os.path.exists(log_training_txt_dir):\n",
    "    os.makedirs(log_training_txt_dir)\n",
    "if not os.path.exists(log_training_excel_dir):\n",
    "    os.makedirs(log_training_excel_dir)\n",
    "\n",
    "# Generate unique file names based on current time\n",
    "log_file          = os.path.join(log_training_txt_dir,   f\"training_keyref2_{current_time}.txt\")\n",
    "excel_file        = os.path.join(log_training_excel_dir, f\"training_keyref2_{current_time}.xlsx\")\n",
    "action_count_file = os.path.join(log_training_txt_dir,   f\"action_count_keyref2_{current_time}.txt\")\n",
    "action_excel_file = os.path.join(log_training_excel_dir, f\"action_count_keyref2_{current_time}.xlsx\")\n",
    "\n",
    "# Define the custom callback -------------------------------------------------------------\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, log_dir, excel_file, txt_file, action_count_file, action_excel_file, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.log_dir = log_dir\n",
    "        self.excel_file = excel_file\n",
    "        self.txt_file = txt_file\n",
    "        self.action_count_file = action_count_file\n",
    "        self.action_excel_file = action_excel_file\n",
    "        self.logs = []\n",
    "        self.episode_rewards = []\n",
    "        self.action_counts = {}\n",
    "        self.episode_start = True\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        # Initialize action counts\n",
    "        self.action_counts = {action: 0 for action in action_list}\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.episode_start:\n",
    "            self.episode_rewards.append(0)\n",
    "            self.episode_start = False\n",
    "\n",
    "        # Record reward for the current step\n",
    "        reward = self.locals['rewards'][0]\n",
    "        self.episode_rewards[-1] += reward\n",
    "\n",
    "        # Increment action count\n",
    "        action = self.locals.get('actions', None)\n",
    "        if action is not None:\n",
    "            action_name = action_list[action[0]]\n",
    "            self.action_counts[action_name] += 1\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        # Called at the end of each episode\n",
    "        sum_reward   = self.episode_rewards[-1] if self.episode_rewards else 0\n",
    "        \n",
    "        self.logger.record('train/episode_reward',   sum_reward)\n",
    "        \n",
    "        self.logs.append({\n",
    "            'sum_reward': sum_reward,\n",
    "        })\n",
    "\n",
    "        self.episode_start = True\n",
    "\n",
    "    \n",
    "    def _on_training_end(self) -> None:\n",
    "        # Save logs to Excel\n",
    "        df = pd.DataFrame(self.logs)\n",
    "        df.to_excel(self.excel_file, index=False)\n",
    "\n",
    "        action_df = pd.DataFrame(list(self.action_counts.items()), columns=['Action', 'Count'])\n",
    "        action_df.to_excel(self.action_excel_file, index=False)\n",
    "\n",
    "        # Save logs to text file\n",
    "        with open(self.txt_file, 'w') as f:\n",
    "            f.write(df.to_string(index=False))\n",
    "        with open(self.action_count_file, 'w') as f:\n",
    "            f.write(action_df.to_string(index=False))\n",
    "\n",
    "# Create the callback\n",
    "callback = CustomCallback(log_dir=logdir, \n",
    "                          excel_file=excel_file,\n",
    "                          txt_file=log_file,\n",
    "                          action_count_file=action_count_file,\n",
    "                          action_excel_file=action_excel_file,\n",
    "                          verbose=1)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "\n",
    "class CustomFeaturesExtractor(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    Custom feature extractor for DQN.\n",
    "    \n",
    "    :param observation_space: (spaces.Box)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space: spaces.Box):\n",
    "        super(CustomFeaturesExtractor, self).__init__(observation_space, features_dim=6)  # Output features_dim matches last layer's output\n",
    "        n_input_nodes = observation_space.shape[0]\n",
    "        self.fc1 = nn.Linear(n_input_nodes, 30)\n",
    "        self.fc2 = nn.Linear(30, 30)\n",
    "        self.fc3 = nn.Linear(30, 30)\n",
    "        self.fc4 = nn.Linear(30, 30)\n",
    "        self.fc5 = nn.Linear(30, 30)\n",
    "        self.fc6 = nn.Linear(30, 6)  # Output layer with 6 nodes\n",
    "\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.tanh(self.fc1(observations))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = F.tanh(self.fc3(x))\n",
    "        x = F.tanh(self.fc4(x))\n",
    "        x = F.tanh(self.fc5(x))\n",
    "        x = self.fc6(x)  # Output layer\n",
    "        return x\n",
    "\n",
    "# Define policy_kwargs for DQN model\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomFeaturesExtractor,\n",
    ")\n",
    "\n",
    "\n",
    "model_path = os.path.join(models_dir, \"DQN_.zip\")\n",
    "# Initialize DQN using the custom model\n",
    "model = DQN(\n",
    "    'MlpPolicy',  # Use a Multi-layer Perceptron (MLP) policy\n",
    "    env,  # Your RL environment\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    buffer_size=1000,  # Replay buffer size N\n",
    "    batch_size=32,  # Batch size\n",
    "    gamma=0.9,  # Discount factor\n",
    "    tau=0.01,  # Soft target update strategy\n",
    "    exploration_initial_eps=0.5,  # Initial epsilon\n",
    "    exploration_final_eps=0.1,  # Final epsilon\n",
    "    verbose=1\n",
    ")\n",
    "model.learn(total_timesteps=8000, \n",
    "            tb_log_name=\"KeyRef2\",\n",
    "            log_interval=10,\n",
    "            reset_num_timesteps=False,\n",
    "            callback=callback)\n",
    "model.save(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------- valid1\n",
      "----- A\n",
      "1443890.0\n",
      "----- B\n",
      "1795000.0\n",
      "----- C\n",
      "3298930.0\n",
      "----------- valid2\n",
      "----- A\n",
      "3689580.0\n",
      "----- B\n",
      "1373530.0\n",
      "----- C\n",
      "2319898.0\n",
      "----------- valid3\n",
      "----- A\n",
      "1108820.0\n",
      "----- B\n",
      "1470690.0\n",
      "----- C\n",
      "3187460.0\n",
      "----------- valid4\n",
      "----- A\n",
      "3310394.666666667\n",
      "----- B\n",
      "2887510.0\n",
      "----- C\n",
      "2418050.0\n",
      "----------- valid5\n",
      "----- A\n",
      "1582290.0\n",
      "----- B\n",
      "8039950.0\n",
      "----- C\n",
      "831920.0\n",
      "----------- valid6\n",
      "----- A\n",
      "5232024.166666667\n",
      "----- B\n",
      "5251580.0\n",
      "----- C\n",
      "3377210.0\n",
      "----------- valid7\n",
      "----- A\n",
      "2341950.0\n",
      "----- B\n",
      "2080293.1428571427\n",
      "----- C\n",
      "6111294.666666667\n",
      "----------- valid8\n",
      "----- A\n",
      "7956090.0\n",
      "----- B\n",
      "9151610.0\n",
      "----- C\n",
      "1691150.0\n",
      "----------- valid9\n",
      "----- A\n",
      "1456290.0\n",
      "----- B\n",
      "8481880.0\n",
      "----- C\n",
      "3028400.0\n",
      "----------- valid10\n",
      "----- A\n",
      "2819677.3333333335\n",
      "----- B\n",
      "3162220.0\n",
      "----- C\n",
      "9034940.0\n",
      "----------- valid11\n",
      "----- A\n",
      "2070960.0\n",
      "----- B\n",
      "8999480.0\n",
      "----- C\n",
      "2932230.0\n",
      "----------- valid12\n",
      "----- A\n",
      "980910.0\n",
      "----- B\n",
      "3729660.0\n",
      "----- C\n",
      "9754930.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "model = DQN.load(model_path, env=env)\n",
    "def softmax_action_selection(model, obs, mu):\n",
    "    obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(model.device)\n",
    "    q_values = model.q_net(obs_tensor).detach().cpu().numpy().flatten()\n",
    "    exp_q_values = np.exp(mu * q_values)\n",
    "    probabilities = exp_q_values / np.sum(exp_q_values)\n",
    "    action = np.random.choice(len(q_values), p=probabilities)\n",
    "    return action\n",
    "\n",
    "results = []\n",
    "method = 'keyref2'\n",
    "InstanceList = [f'valid{i+1}' for i in range(12)]\n",
    "ScenarioList = ['A', 'B', 'C']\n",
    "\n",
    "mu = 1.6 \n",
    "for instance_id in InstanceList:\n",
    "    print(\"-----------\", instance_id)\n",
    "    for scenario_id in ScenarioList:\n",
    "        print(\"-----\", scenario_id)\n",
    "        # Reset the environment with the new dataset\n",
    "        env.reset(test=True, \n",
    "                  datatest=instance_id, \n",
    "                  scenariotest=scenario_id)\n",
    "        \n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = softmax_action_selection(model, obs, mu)\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        tardiness = env.calc_tardiness()\n",
    "        print(tardiness)\n",
    "        results.append({\n",
    "                        'Method'    : method,\n",
    "                        'InstanceID': instance_id,\n",
    "                        'ScenarioID': scenario_id,\n",
    "                        'Tardiness' : tardiness\n",
    "                        })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "file_name = f\"VALIDATION/keyref2.xlsx\"\n",
    "df.to_excel(file_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dunnbebes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
